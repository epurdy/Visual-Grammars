\marginnote{beginning of structure/ex\_constituency.tex}

\subsection{Figure out optimal single-example grammar}

We use explicit correspondences to learn the statistically best set of
constituents when building a grammar from a single
example. Specifically, since the points on each curve from the
hand-annotated Romer dataset correspond semantically, we can ask which
decomposition of the curves yields the model which gives the highest
log-likelihood to the curves seen. This can be done with a simple
dynamic program:
\begin{align*}
COST[i,k] &= \min_j COST[i,j] + COST[j,k] - \min_{\mu, \kappa} \sum_a \log Watson(C_a[i],C_a[j],C_a[k]; \mu, \kappa)\\
\intertext{for $k\ne i, (i+1) \mod n$, and}
COST[i,(i+1) \mod n] = 0
\end{align*}
Minimizing the sum of negative log-likelihoods of the Watson
distribution is simply maximum likelihood estimation for the Watsone
distribution, which is explained in Section
\ref{sec-learning-triangle}.

% There is a strange issue here, but I've seen it before in other code
% and I don't think it's a bug. Converting to Bookstein coordinates
% improves the results (or here, the results are more intuitive to me),
% even though the Watson distribution shouldn't need this.

The constituents selected by the algorithm are shown in Figure
\ref{fig-constituents-optimal} Note that the arms are found as
constituents. The constituents that seemed most intuitive to me are
shown in Figure \ref{fig-constituents-handpicked}. The main difference
between the two is that the algorithm does not select balanced
decompositions of object sub-parts that show relatively little
geometric variation, instead preferring to parse long subcurves as a
single line segment combined with a slightly shorter long subcurve.

\begin{figure}
%\includegraphics[width=\linewidth]{experiments/6.structure/constituents/output.d/optimal.png}
\caption[Finding optimal constituents]{Finding optimal constituents. This figure generated by the experiment experiments/6.structure/constituents.}
\label{fig-constituents-optimal}
\end{figure}

\begin{figure}
%\includegraphics[width=\linewidth]{experiments/6.structure/constituents/output.d/handpicked.png}
\caption[Hand-picked constituents]{Hand-picked constituents.}
\label{fig-constituents-handpicked}
\end{figure}

% \subsection{Constituency from decay}

% We have the following curve:

% \begin{figure}
% \includegraphics[width=\linewidth]{experiments/6.structure/constituency_heuristics/output.d/curve.png}
% \end{figure}

% We attempt to decompose the curve meaningfully by iteratively
% simplifying the curve, like this:

% \begin{figure}
% \includegraphics[width=\linewidth]{experiments/6.structure/constituency_heuristics/output.d/decay.png}
% \caption[Constituency heuristics]{We iteratively simplify the curve. This figure generated by the experiment experiments\/6.structure\/constituency\_heuristics.}
% \end{figure}

% Here is the decomposition found using this heuristic.

% \begin{figure}
% \includegraphics[width=\linewidth]{experiments/6.structure/constituency_heuristics/output.d/decay_sdf.png}
% \caption[Constituency heuristics]{Decomposition found. This figure generated by the experiment experiments\/6.structure\/constituency\_heuristics.}
% \end{figure}

\marginnote{end of structure/ex\_constituency.tex}
