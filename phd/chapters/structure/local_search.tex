
\marginnote{beginning of structure/local\_search.tex}


\subsection{Creating Reusable Parts with Replacement}
\label{sec-replacement}

Given a grammar $\GGG$, we wish to create a grammar which still
explains our training data, and is simpler. We would also like to
create grammars that have reusable parts, as explained in
\cite{amit-bernstein}. This can be done using
the operation of \emph{replacing $Y$ with $X$}: whenever $Y$ is on the
right-hand side of a rule, we replace it with $X$. Whenever $Y$ is on
the left-hand side of a rule, we delete that rule. This may lead to a
merging of two rules $[Z_1 \to X Z_2], [Z_1 \to Y Z_2]$, in which case
we combine the multinomial probabilities and average the midpoint
distributions.

Why does this operation create reusable parts? Ideally, before the
replacement, $Y$ and $X$ are nonterminals that are used to parse
similar curves, but which fit differently into the larger
grammar. After the replacement, these curves will be parsed only with
$X$, and $X$ will fit into the larger grammar in two different
ways. $X$ has thus become a reusable part.

Note that replacement is asymmetric. This could be good; since
approximate similarity is not transitive, we would like to replace
nonterminals $X_1,\dots, X_k$ with whichever $X_i$ is most
representative.

This transformation should be applied when the nonterminals $X$ and
$Y$ have similar \emph{internal} structure. Intuitively, this means
that any curve generated by $X$ can be generated by $Y$ with a similar
probability, and vice versa. We formalize this notion by measuring the
KL divergence of the respective \emph{inside distributions}. The
inside distribution of $X$ is the distribution over curves that comes
from sampling in the standard way. We call this distribution the
inside distribution because its probabilities are calculated in the
first pass of the Inside-Outside Algorithm, as explained in Section
\ref{sec-parsing}.

\subsection{Principled Reason for using the KL Divergence}

If we think of $X$ as stealing the data $d_Y$ that $Y$ was explaining,
then we'll be paying $\log \frac{P_Y(d_Y)}{P_X(d_Y)}$. If we don't
know $d_Y$, then we'll just have to guess, so we'll be paying $E_Y
\left[ \log \frac{P_Y(d_Y)}{P_X(d_Y)} \right]$, which is exactly the
KL divergence.

If the formula were correct, this would be weighted by the actual
amount of data that $Y$ is explaining, but we can maybe assume that
this is uniform pre-training. Or weight the KL divergence term by how
much data we think $Y$ should explaining.

\subsection{Creating Choice with Merging}
\label{sec-merge}

We want to create grammars that have choice, as described in Section
\ref{sec-structural}. This can be done by the operation of \emph{merging $X$ and
  $Y$}: we replace every instance of $Y$ in either the left or
right-hand side of a rule with $X$. This may lead to a merging of two
rules $[X \to Z_1 Z_2], [Y\to Z_1,Z_2]$ (and some other cases). When
this happens, we combine the multinomial probabilities and average the
midpoint distributions of the rules.

As a heuristic to guide merging, we plan to apply the approach of the
last section, but to use the outside probability instead of the inside
probability. If two nonterminals $X$ and $Y$ have similar outside
probabilities on every subcurve, then $X$ and $Y$ can be said to have
similar \emph{outer structure}, since the grammar is placing them in
similar contexts. Intuitively, this means that $X$ and $Y$ are
interchangeable, so that for any curve in which $X$ is used to explain
a subcurve, there is another equally likely curve where that subcurve
has been replaced with a subcurve that can be explained by $Y$.

\subsection{Searching over grammars}

In the light of \cite{stolcke}, it should be the case that we can
reach any grammatical structure if we start from a sufficiently rich
initial grammar on every training curve, and apply replacements,
merges, and deletions. It would be good to check this.

We do not know of any optimal way to search over grammatical
structures, so our proposal is to start from the initial grammars
described in Section \ref{sec-single}, and proceed by greedily applying the
transformations listed. We could also try a slightly more flexible
search algorithm, such as the beam search used by \cite{stolcke}.

\subsection{Possible Tricks to Speed Up Search}
We want to search over the space of grammars using the posterior
probability of the grammar. Unfortunately, knowing the posterior
probability requires parsing every single input curve with a
potentially very large grammar.

In \cite{stolcke}, grammar induction was sped up by only occasionally
reparsing the data. This was done under the assumption that
transformations applied to a grammar don't change the Viterbi parses
in a non-obvious way. (When applying replacements and merges to a
grammar, it is reasonable to assume that the optimal parse of a curve
usually changes in the same way as the grammar does.)

Such an approach would allow a search algorithm to run much faster,
possibly with little effect on performance. This is impossible to do
with full inside-outside parsing, but is probably straightforward with
Viterbi parsing.

\marginnote{end of structure/local\_search.tex}