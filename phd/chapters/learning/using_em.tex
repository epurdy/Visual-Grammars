\marginnote{Beginning of using\_em.tex}

For all the experiments in this section, we build different grammars
from the example curve in Figure \ref{fig-romer-example}. We then
retrain the grammar in various ways using the training set shown in
Figure \ref{fig-romer-train}. We show samples from each model in order
to demonstrate what has been learned. We want models to generate
reasonable-looking human silhouettes, and ideally generate some that
differ significantly from any training example. For instance, we would
like the model to learn that limbs can move independently of one
another.

% \begin{figure}
% \includegraphics[width=\linewidth]{experiments/3.learning/simple_tuning/output.d/examples.png}
% \caption{The example curve, from which we build a grammar with hand-chosen rules.}
% \label{fig-romer-example}
% \end{figure}

% \begin{figure}
% \includegraphics[width=4in]{experiments/3.learning/simple_tuning/output.d/training.png}
% \caption{Training set}
% \label{fig-romer-train}
% \end{figure}


\section{Simple Tuning of hand-built grammar with curves of constant length}

First, we use EM to train an initial grammar that has no choices. Our
grammar is produced from a hand-chosen decomposition. Samples from the
grammar after various numbers of rounds of training are shown
below. Since we are using unimodal midpoint distributions, and our
grammar has no choice, all the samples are slight variations on one
particular mean shape. The mean shape chosen is a very reasonable one,
although the arms are longer than is ideal.

We repeat this grammar three times with different initial grammatical
structures, to show how the performance of the algorithm depends on
our initial structure. We also show the log-likelihoods of the data
after each round of training in the table below.

\input{output/3.learning/simple_tuning/out}

\section{Tuning with multiple midpoints, and curves of constant length}

In the next experiment, we enrich the grammar by adding in several
copies (in this case, five) of each rule, with jittered midpoints. (If
we used the same midpoint for each copy of the rule, parses could use
the rules interchangeably, and EM would never break this symmetry.)
Below, we show samples from the grammar after various numbers of
rounds of training. The samples show significantly more variation than
in the previous experiment, but they also include many ill-formed and
unlikely shapes. This is because the grammar cannot model correlations
between levels, so that e.g., it cannot pick the location of the elbow
based on the location of the hand, which means that the arm is often
ill-formed and self-intersecting.

We repeat this grammar three times with different initial grammatical
structures, to show how the performance of the algorithm depends on
our initial structure. We also show the log-likelihoods of the data
after each round of training in the table below.

\input{output/3.learning/multi_tuning/out}

\section{Tuning with multiple midpoints, learning multi-level correlations}

In this experiment, we enrich the grammar by adding in several copies
of each nonterminal, each of which has several copies of the original
rule with jittered midpoints. If our original rule was $X\to YZ$, then
we have five copies each of $X,Y,Z$, and each $X_i$ has five rules of
the form $X_i \to Y_j Z_k$, where $j$ and $k$ are chosen independently
at random.

Below, we show samples from the grammar after various numbers of
rounds of training. The silhouettes look much better than in the
previous experiment. This is because we have learned correlations
between levels, i.e., the choice of rule at a higher level influences
the choice of rule at a lower level.

We repeat this grammar three times with different initial grammatical
structures, to show how the performance of the algorithm depends on
our initial structure. We also show the log-likelihoods of the data
after each round of training in the table below.

\input{output/3.learning/correlated_tuning/out}

\section{Full Tuning}

In this experiment, we build a grammar from the example curve that
allows all possible decompositions, and then train using EM. Below we
show samples from the grammar after five and ten rounds of
training. Note that there is no longer a choice of initial structure.

\input{output/3.learning/full_tuning/out}

% \section{Learning Texture}
% \input{output/3.learning/scaled_nts/out}

\marginnote{End of using\_em.tex}
