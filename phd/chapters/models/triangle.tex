
\marginnote{beginning of models/triangle.tex}

When sampling from our grammar, we place a midpoint $q$ according to
the midpoint distribution $\mu_{X\to YZ}(q; p, r)$, where $X_{p,r}$ is
the placed symbol we are currently replacing, and $X\to YZ$ is the
substitution rule we are using.

When building a grammatical model from a single curve $C$, we base the
distribution $\mu_{X^{(i,k)}\to X^{(i,j)}X^{(j,k)}}$ on the triangle
$C[i], C[j], C[k]$, with $C[i],C[j],C[k]$ playing the roles of $p,q,$
and $r$ respectively.

We have experimented with two different approaches to modeling the
shape of triangles, the Watson distribution and a non-parametric
distribution. Unless otherwise specified, midpoint distributions will
always be modeled with the Watson distribution.

\subsection{Procrustes Distance and The Watson Distribution}

One useful midpoint distribution is given by the complex Watson
distribution, which is a probabilistic analogue of the Procrustes
distance.

Let us represent our points $(x,y)$ in $\RR^2$ as complex numbers
$x+iy$, and ordered triples of points by vectors of length three. For
every such vector $z=\left(\begin{array}{c c c} p& q&
    r\end{array}\right)$, we define a canonical version $$\widehat{z}
= \frac{z - w}{\| z - w \|},$$ where $w =
\left(\begin{array}{c c c} \frac{p+q+r}{3}& \frac{p+q+r}{3}&
    \frac{p+q+r}{3} \end{array}\right)$. We do this transformation to
achieve invariance to scale and rotation, since, if $y = az + b$,
$\widehat{y}=\widehat{z}$.

The (complex) Watson distribution is then defined as follows: 
$$Watson(z; \mu,\kappa) =
\frac{1}{Z(\kappa)}e^{\kappa | \widehat{z}^*\widehat{\mu}|^2},$$ where
$\mu$ is the mode of the distribution, and $\kappa>0$ is a
concentration parameter. More details on the Watson distribution can
be found in \cite{mardia-dryden}.

The Watson distribution can be thought of as a probabilistic version
of the Procrustes distance, which can be defined as
$$d_P(y,z) = \sqrt{1- \frac{z^* y y^* z}{y^* y z^* z}},$$
and thus 
$$Watson(z; \mu, \kappa) \propto e^{-\kappa \cdot d_P(\widehat{z}, \widehat{\mu})^2},$$
where we are using the fact that $\widehat{z}^*
\widehat{z} = \| \widehat{z} \|^2 = 1$.

\subsection{Non-Parametric Deformation Model}

We have also experimented with the nonparametric distribution given by
the Parzen windows method \cite{parzen}. We want our grammar to be
invariant to simultaneous translation, scale, and rotation of the
points $p,q,r$. Therefore, we translate, scale, and rotate $\RR^2$
with a map $\phi$ such that $\phi(p) =(0,0)$ and $\phi(r) =(1,0)$, and
represent $q$ via the coordinates $\widehat{q} = \phi(q)$.

If we have seen samples $q_1, \dots, q_k$, then
$$\mu_{X\to YZ}(q ; p, r) = \frac{1}{n} \sum_{i=1}^{k}
\frac{1}{2\pi h^2} e^{\frac{\| \widehat{q_i} - \widehat{q}\|^2}{2h^2}}.$$

In the context of kernel density estimators, the parameter $h$ is
called the bandwidth. It specifies how much smoothing to apply to the
density estimation. Selecting a suitable bandwidth is often
problematic. 

It is worth noting that this nonparametric distribution is just a
mixture of Gaussians, and thus there is no solid distinction between a
mixture of multiple copies of the rule $X\to YZ$ with Gaussian
midpoint distributions, and a single copy with a mixture of Gaussians
midpoint distribution. We will prefer the second, since the resulting
grammar has a simpler structure, as discussed in Chapter \ref{chap-structure}.


\subsection{Open Questions}

Which shape grammars can be converted to Chomsky Normal Form?

This is not as straightforward as the corresponding question for
traditional context-free grammars, since not all control point
distributions can be expressed as products of midpoint
distributions. Recursion makes this question more subtle.

\newthought
Process Model of Continuous Curves

We can imagine a shape grammar which does not have any terminal
$\ell$, but rather continues to generate sample points forever. If
such a grammar is finite, it would have to have recursive rules to
allow an infinite number of expansions. The simplest case would be
rules of the form $L\to LL$.

If we have a grammar with only one symbol $L$, and only one rule $L\to
LL$, with $\mu_{L\to LL}(q;p,r)$ being the degenerate distribution
that only takes on the value $q=\half(p+r)$, then it is clear that we
generate only straight lines.

We can ask what conditions on the grammar are necessary to produce
smooth curves under such a model. For specificity, let us assume that
we represent our curve by a function $C(t) : [0,1] \to \RR^2$. Let
$C_k$ be the curve we obtain after $k$ rounds of substitution, and let
$C(\frac{i}{2^k}) = C_k[i]$. It can be checked that this gives the
same value for $\frac{i}{2^k}$ and $\frac{i'}{2^{k'}}$ when the two
are equal, and thus $C(\cdot)$ is well-defined.  Since the set
$\left\{\frac{i}{2^k}\right\}$ is dense in $[0,1]$, we can define
$C(t)$ for other values by continuity when $C(\cdot)$ is continuous.

Then, there are several interesting questions:

\begin{q}
What conditions on the grammar are necessary in order for $C(t)$ to
be continuous?
\end{q}

\begin{q}
What conditions on the grammar are necessary in order for $C(t)$ to
be smooth?
\end{q}

\begin{q}
Is it possible to write down a simple grammar for a simple parametric
curve such as a circle or a Bezier curve?
\end{q}

\begin{q}
Suppose that we generate a smooth curve $C(t)$ via a grammar \GGG, and
then subsample it to get a curve $C_*$ with a finite number of
points. How do we parse $C_*$ with \GGG, and how do we use this parse
to assign a likelihood to $C_*$ under \GGG?
\end{q}

\begin{q}
What is a good probabilistic interpretation of subsampling?
\end{q}

\marginnote{end of models/triangle.tex}