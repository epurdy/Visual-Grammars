\marginnote{beginning of models/models\_intro.tex}

We are building probabilistic models of shape. For now, we consider
only models of shape that deal with the location in the plane of a
fixed number of (ordered) landmarks $z_1, \dots, z_n$, which in
our case will define a curve outlining some shape. Some existing
approaches to this problem are: Markov models, the Procrustes distance
(and its probabilistic counterpart, the Watson distribution),
indepedent Gaussians, and independent non-parametric distributions.

We would like to prove that our models do as well or better in
explaining the geometric variability of various shape classes.
The easiest way to compare two generative probabilistic models is to
examine samples from them, and subjectively assess their similarity to
real data. This is inherently a very qualitative evaluation.

Our main goal in building shape models is to allow us to do inference
on new curves. We can calculate a likelihood that a shape class
generated a particular curve. We start by considering models defined
by perturbations of a single input curve.

When comparing to other probabilistic models, we can evaluate them in
a more quantitative way using (estimated) cross-entropy, which is
defined as
$$H(\{x_1,\dots,x_n\}, q) = - \sum_{i=1}^n \frac{1}{N} \log_2 q(x_i).$$
If $X = \{x_1,\dots,x_n\}$ is unseen data, and a large enough sample
to be statistically useful, then $H(X,q)$ is a good measure of how
well a model explains unseen data. Smaller values of $H(X,q)$ suggest
that $q$ is a better model. One problematic aspect of cross-entropy is
that it is only meaningful if $q$ is a correctly normalized
probability distribution. Often it is easier to compute
$\widetilde{q}(x) = Z\cdot q(x)$ for some unknown but fixed constant
$Z$.

The most straightforward model for perturbing a curve is a Markov-type
model. A curve is a sequence of line segments
$\ell_1,\dots,\ell_k$. We can describe the curve by giving the length
$l_i$ of each $\ell_i$, and the angle $\theta_i$ between $\ell_i$ and
$\ell_{i+1}$. If we perturb each $l_i$ and $\theta_i$ slightly, we
get a curve which differs from the original. Some samples from this
are shown in Figure \ref{fig-markov}.
\begin{figure}
  \centering
\includegraphics[width=120mm]{images/markov_new.png}
\caption{The problem of drift makes a Markov model unappealing. Random
  samples from this model are too similar locally and too dissimilar
  globally. These shapes were generated by changing each length $l$ by
  a multiplicative factor of $1 + \NNN(0,\sigma)$, and changing each
  angle $\theta$ by adding $\pi \cdot \NNN(0,\sigma )$. Here $\sigma$
  is the value listed underneath the shape.}
\label{fig-markov}
\end{figure}

\begin{figure}
  \centering
\includegraphics[width=120mm]{images/hcm.png}
\caption{The shape on the left is the original, and the other curves
  have been produced by sampling from the hierarchical curve models of
  . The model produces curves which have more perceptual
  similarity than the Markov model. }
\label{fig-hcm}
\end{figure}
\footnote{\cite{hcm}}

The major weakness of the Markov model is \emph{drift}: the small
perturbations will accumulate, and the overall shape of the curve will vary
greatly. A straight line has some probability of curling into a tight
spiral. Consider a shape like a hand: a hand has fingers that
protrude. This means that there are two points (namely the two points
where a finger meets the rest of the hand) far away in the curve that
we always expect to be very close together. A Markov perturbation of
the shape is likely to pull these points far apart.

To defeat this problem, \emph{hierarchical curve models} were
introduced in \cite{hcm}. There, the following curve model is given:
\begin{itemize}
\item Given a model curve $C$, decompose $C$ hierarchically by
  repeatedly cutting it in half, in a balanced but otherwise arbitrary
  fashion.
\item Suppose our first decomposition is $C=DE$. We perturb $C$ into
  $C'$ by first perturbing the midpoint of $C$ slightly. We then
  rotate and scale the curves $D$ and $E$ so that $D'$ goes from the
  endpoint of $D$ to the new midpoint, and $E'$ goes from the new
  midpoint to the endpoint of $E$.
\item We then recursively apply the same process to each subcurve $D$
  and $E$. 
\end{itemize}
It is easy to see that this defeats the problem of drift, because the
overall shape of the curve is determined in a constant number of
substitutions from the beginning curve. If our perturbations are
smaller for larger curves, then we will leave the overall shape very
similar while allowing significant local variation. Some samples from
this model are shown in Figure \ref{fig-hcm}.

Hierarchical curve models have room for improvement, as can be seen in
Figure \ref{fig-hcm}. The variations do not respect the perceptual
structure of the original curve; in particular, we do not see
articulated parts being articulated.

In this document, we describe a grammatical reformulation of the work
of \cite{hcm}, which we hope will improve upon it in two
ways. Firstly, we hope to allow for structural variation, which we
argue is an important goal for computer vision in Section
\ref{sec-structure-intro}. Secondly, we give a generative probabilistic
model of curves, which allows us to retrain the parameters of a
grammar in a mathematically sound way, rather than optimizing many
parameters in an ad-hoc manner (which will tend to be expensive and
fragile).

\subsection{Classification}

When we are comparing to a non-probabilistic model, sampling is
impossible, and the cross-entropy is undefinable. In this case, the
most straightforward way to compare models is by using them both in
some task like classification. For instance, we can use both models to
classify leaves as being either oak or maple leaves. Whichever model
has a lower error rate is better at capturing the differences between
the two classes of shapes.  Our generic approach to classification
will be to build a probabilistic model for each class, and assign a
given curve to the class whose model gives it the highest likelihood.

We would also like to build models that deal effectively with curves
that have varying numbers of points. This is important to achieve
scale invariance, since larger objects will generally have more
detailed boundaries. There are few standard probabilistic models of
curves with varying numbers of points, but there are many
discriminative models that deal with such curves.

% Unfortunately, there are few classification tasks hard enough to
% distinguish between different models. 
There are more (and harder) classification tasks when we consider the
more restricted problem of comparing two different shapes, rather than
modeling an entire class and classifying a single shape. One
relatively difficult matching task is MPEG-7. We are only allowed to
compare images in pairs, and there are about 2 million pairs that must
be iterated over.

\marginnote{The LabelMe dataset \cite{labelme} also has some adequate
  user-drawn polygons for many classes of natural shapes.}

\marginnote{end of models/models\_intro.tex}
