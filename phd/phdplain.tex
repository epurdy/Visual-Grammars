\documentclass{book}
%% \documentclass{tufte-book}
\usepackage{leonine,cancel,amsmath,amssymb,amsthm,graphicx, setspace}%%xy, setspace, amscd (commutative diagram)
\usepackage{grffile, algorithm, algorithmic}

\usepackage{caption, subcaption}

\newcommand{\marginnote}{\mar}
\newenvironment{marginfigure}{\begin{figure}}{\end{figure}}
\newcommand{\newthought}{\vspace{12pt}}
\usepackage{placeins, color}

\newcommand{\bow}[1]{\colorbox{black}{\color{white} #1}}
\newcommand{\experiment}[1]{\bow{This section's results can be recreated by running #1}}

\def\tne{=\joinrel\mathrel|}
\newcommand{\algorithmicinput}{\textbf{Input:}}
\newcommand{\INPUT}{\item[\algorithmicinput]}

\newcommand{\tree}[3]{\frac{#1}{#2 \qquad \mid \qquad #3}}
\newcommand{\treee}[4]{\frac{#1}{#2 \qquad \mid \qquad #3 \qquad \mid \qquad #4}}

\newcommand\infrul[1]{\ensuremath{\frac{\qquad \qquad}{\qquad\qquad}\quad #1}}


\newcommand\spk[2]{\includegraphics[width=#1mm]{images/#2.png}}
%% \newcommand\spk[2]{\framebox{images/#2.png}}


\title{ Grammatical\\*Methods in\\*Computer Vision\\*and\\*Machine Learning}
%% \author[Eric Purdy]{Eric Purdy}
\author{Eric Purdy}

% \publisher{
% A DISSERTATION SUBMITTED TO\\
% THE FACULTY OF THE DIVISION OF THE PHYSICAL SCIENCES\\
% IN CANDIDACY FOR THE DEGREE OF\\
% DOCTOR OF PHILOSOPHY\\
% \ \\
% DEPARTMENT OF COMPUTER SCIENCE
% }
% \publisher{Submitted to Satisfy the Requirements for a Ph.D. at the University of Chicago}

%\usepackage{microtype}
\usepackage{booktabs}% For nicely typeset tabular material

% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%%
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}

% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

% Generates the index
\usepackage{makeidx}
\makeindex

\begin{document}

% Front matter
\frontmatter

% v.2 epigraphs
% \newpage\thispagestyle{empty}
% \openepigraph{%
% The public is more familiar with bad design than good design.
% It is, in effect, conditioned to prefer bad design, 
% because that is what it lives with. 
% The new becomes threatening, the old reassuring.
% }{Paul Rand%, {\itshape Design, Form, and Chaos}
% }


% \newpage\thispagestyle{empty}
% \openepigraph{%
%   Penetrating so many secrets, we cease to believe in the
%   unknowable. But there it sits nevertheless, calmly licking its
%   chops.}{H. L. Mencken}

% ... they are ill discoverers that think there is no land when they can see nothing but sea.
%     Francis Bacon (1561-1626) English essayist, philosopher, statesman.

% r.3 full title page
\maketitle

% r.5 contents
\setcounter{tocdepth}{2}
\tableofcontents

\listoffigures

\listoftables

% r.7 dedication
% \cleardoublepage
% ~\vfill
% \begin{doublespace}
% \noindent\fontsize{18}{22}\selectfont\itshape
% \nohyphenation
% Dedicated to those who appreciate \LaTeX{} 
% and the work of \mbox{Edward R.~Tufte} 
% and \mbox{Donald E.~Knuth}.
% \end{doublespace}
% \vfill
% \vfill

\chapter{Notation}
  \label{chap-notation}

  We denote $x$ etc.

  \section{Style Guide}
  \input{chapters/styleguide}

% r.9 introduction
\cleardoublepage
\mainmatter

\chapter{Introduction}
  \label{chap-intro}
  \input{chapters/intro}

\chapter{Grammatical Models of Shape}
  \label{chap-models}

  \section{TODO}
    \input{chapters/models/models_wishlist}  

  \section{Curve Models}
    \label{sec-models-intro}
    \input{chapters/models/models_intro}  

  \section{Motivating Example: L-Systems}
    \label{sec-models-lsystems}
    \input{chapters/models/lsystems}

  \section{Dealing with Curves}
    \label{sec-models-curves}
    \input{chapters/models/curves}

  \section{Stochastic Shape Grammars: A Generative Model for Curves}
    \label{sec-models-curvegrammars}
    \input{chapters/models/curvegrammars}

  \section{Example Grammars}
    \label{sec-models-examples}
    \input{chapters/models/hand_built}

  \section{Parsing with Grammars}
    \label{sec-models-parsing-em}
    \input{chapters/models/parsing_em}

  \section{Building a Grammar From a Single Curve}
    \label{sec-models-singlecurve}
    \input{chapters/models/single_curve}

  \section{Models of Triangle Deformation}
    \label{sec-models-triangle}
    \input{chapters/models/triangle}

  \section{Dealing with Variation in Length}
    \label{sec-models-dirichlet}
    \input{chapters/models/scale_dirichlet}

  \section{Parsing to Recover Correspondences}
    \label{sec-models-parsing}
    \input{chapters/models/parsing}

  \section{Comparing to Independent Gaussians}
  The following three images show samples from a model using an
  independent Gaussian distribution for each point. They are trained
  on the Romer dataset introduced in the learning chapter.

  Respectively, they show samples from the maximum likelihood model,
  samples from the maximum likelihood model with no variance, and
  samples from the maximum likelihood model with its variance
  artificially decreased. It is plain to see that these models are
  unable to capture much interesting structure. Compared to the
  samples at the end of the learning chapter, these are very bad.

    \input{output/1.models/comparison_gaussians/out}

  \section{Comparing to Independent Nonparametric Distributions}
  The following images show samples from a model using independent
  nonparametric distributions for each point. They are trained on the
  Romer dataset introduced in the learning chapter. The independence
  assumption is clearly not a good fit for modeling these curves.    

    \input{output/1.models/comparison_parzen/out}


\chapter{Detecting Objects in Cluttered Images}
  \label{chap-detection}

  \section{TODO}
    \input{chapters/detection/detection_wishlist}  

  \section{Introduction}
    \label{sec-detection-intro}
    \input{chapters/detection/detection_intro}

  \section{Goals}
    \label{sec-detection-goals}
    \input{chapters/detection/detection_goals}

  \section{Prior Work}
    \label{sec-detection-priorwork}
    The problem of finding objects in the plane with recursive costs
    defined by a hierarchical model has been studied before in
    \cite{jin-geman, grammar-tr, pictorial}. 

    Admissible coarse-to-fine methods have been studied before in
    \cite{astar, cfdp}.

%   \section{Motivating Examples}
%     \input{chapters/detection/detection_motivation}

%  \section{Speeding up Detection with Filtrations}
    \input{chapters/detection/filtrations}

  \section{Data Models for Object Detection}
    \label{sec-detection-datamodel}
    \input{chapters/detection/datamodel}

  \section{Parsing Scenes}
    \label{sec-detection-scenes}
    \input{chapters/detection/scenes}

  % \section{The Problem of Unintended Reuse}
  %   \label{sec-detection-reuse}
  %   \input{chapters/detection/reuse}

  \input{chapters/detection/local}

  % \section{Experimental Results}
  %   \label{sec-detection-experiments}

% \begin{figure*}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.2.d/thefinalparse.png}



% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.2.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.2.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.3.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.3.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.3.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.4.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.4.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.4.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.5.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.5.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.5.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.6.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.6.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.6.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.7.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.7.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.7.d/local.x5.interior.png}
% \end{figure*}
% \begin{figure*}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.8.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.8.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.8.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.9.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.9.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.9.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.10.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.10.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.10.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.11.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.11.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.11.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.12.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.12.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.12.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.13.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.13.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.13.d/local.x5.interior.png}
% \end{figure*}
% \begin{figure*}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.14.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.14.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.14.d/local.x5.interior.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.15.d/thefinalparse.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.15.d/local.x5.orientations.png}
% \includegraphics[width=0.3 \linewidth]{output/2.detection/image_parsing/out.15.d/local.x5.interior.png}
% \end{figure*}

% \input{experiments/4.images/standard_cky/standard_cky}
% \input{experiments/4.images/fuzzy_cky/fuzzy_cky}
% \input{experiments/4.images/image_parsing/image_parsing}



% \section{Probabilistic Derivation Problems}

% \marginnote{Figure this out}

% In general, we will be interested in probabilistic interpretations of
% lightest derivation problems, where the probability of a derivation is
% defined to be the product of the probability of each inference rule
% used in the derivation. If we consider the weight of a rule to be the
% negative log of its probability, then this formulation is equivalent
% to the lightest derivation problem.



\chapter{Parameter Learning for Grammatical Models}
  \label{chap-learning}

  \section{TODO}
    \input{chapters/learning/learning_wishlist}  

  \section{Goals}
    \label{sec-learning-goals}
    \input{chapters/learning/learning_goals}

  \section{Prior Work}

  \section{Motivating Examples}

  \section{Learning a Model of Triangles}
    \label{sec-learning-triangle}
    \input{chapters/learning/triangle}

  \section{Learning Grammar Parameters with the EM Algorithm}
    \label{sec-learning-parsing}

    \input{chapters/learning/doing_em}

    \marginnote{Merge the following with the previous. The subsection
      ``Scale-dependent Dirichlet Prior'' should have much of its
      material moved to the Models chapter, since it is largely about
      selecting an initially plausible set of parameters.}

    \input{chapters/learning/parameters}

  \section{Practical Use of the EM Algorithm with Curve Grammars:
    Obstacles and Strategies}
    \label{sec-learning-usingem}
    \input{chapters/learning/using_em}

%     \marginnote{Can slide naturally into talking about discriminative
%       and shadowcasting learning. Train, sample, and then hand-tag
%       some samples as bad, and then retrain with them weighted
%       negatively. (And probably with good samples weighted positively.)}

%   \section{Shadowcasting Priors}
%     \input{chapters/learning/shadowcasting}

\chapter{Structure Learning for Grammatical Models}
  \label{chap-structure}

  \section{TODO}
    \input{chapters/structure/structure_wishlist}  

  \section{Introduction}
    \label{sec-structure-intro}
    \input{chapters/structure/structure_intro}

  \section{Goals}
    \input{chapters/structure/structure_goals}

  \section{Prior Work}

  \section{Motivating Examples}
  \marginnote{Probably want to move stuff from the introduction to
    here.}

  \section{What Characterizes an Optimal Structure?} 
    \label{sec-structure-optimality}
    \input{chapters/structure/optimality}

  \section{Incrementally Incorporating Novel Structures}
  \marginnote{This is buried inside of singleuse.tex, which is
    unincorporated.}

  \section{How do we do Local Search for an Optimal Structure?}
    \input{chapters/structure/local_search}

  \section{Approximating the KL Divergence Efficiently}
    \input{chapters/structure/kl}

  \section{How do we Identify Natural Constituents?}
    \input{chapters/structure/constituency}
    \input{chapters/structure/ex_constituency}

  \section{Can we Learn a Model of Curve Texture?}
    \input{chapters/structure/texture}

\chapter{Approximate Parsing of One-Dimensional Signals}
  \label{chap-sdf}

  \section{Goals}
    \input{chapters/sdf/sdf_goals}

  \section{Prior Work}

  \section{Motivating Examples}

  \section{Constructing Sparse Decomposition Families}

  \section{Hierarchical Matching Costs}

  \section{Experimental Results}

  \section{Rest is Unincorporated}

  \section{SDF's}
    \input{chapters/sdf/sdf}

  \section{SDF's, part 2}
    \input{chapters/sdf/sdf2}

% \chapter{Unincorporated Material}

%   \section{Weird stuff}
%     \input{chapters/singleuse}
%     \input{chapters/deformation}
%     \input{chapters/structure_notes}
%    \input{chapters/parsing}

%   \section{Experimental Plans}
%     \input{chapters/plans}

%   \section{Datasets}
%     \input{chapters/datasets}
%     \input{experiments/0.datasets/synth/synth}
%     \input{experiments/0.datasets/romer/romer}
%     \input{experiments/0.datasets/leaves/leaves}





%%
% The back matter contains appendices, bibliographies, indices, glossaries, etc.
\backmatter

% \bibliography{phd}
\bibliography{phdplain}
\bibliographystyle{plainnat}

\printindex

\end{document}

