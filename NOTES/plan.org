* TODO for today
  - finish annotating romer I
  - sample from local_nts somehow
  - figure out how to fit differently constrained watsons, e.g.,
    watson with fixed mean, watson with mean constrained to lie on a
    line, etc.  
  - figure out how to do the shared scaled LLL rules

* notes on code organization
  - in general, each file should do one thing. right now there is too
    much of a tendency towards monolithic objects.

  - simplify the code relating to abstract grammars, such that there
    is only one (possibly parameterized) type of grammar. make it so
    that sdf and shape grammar both include a grammar, in a more
    straightforward way
    - as part of this, maybe take in some functions that do particular
      things on the grammar?

    - at some point, would be good to solve the start/0 issue

    - at some point, would be good to have it figure out the correct
      ordering of symbols etc. instead of having to do things in a
      particular order

  - general rule of thumb(?) - the library files should not have
    serious choices in them, they should give enough support for the
    experiments and executables to make choices. when a choice is
    needed, take a relatively generic function instead of various
    parameters. this is good for keeping the library current and
    correct, and as long as we don't change the sort of function we
    accept, it also means that old experiments will still run, even if
    we have moved on to different choices in newer experiments

  - rename curve_network maybe? think about the data structure in there

  - think about moving geometry into basically a module about complex #s

  - shape.ml is kind of a flaky file... there's very little in it
    though, so maybe that's ok

* METRICS
  - examine samples
  - examine pictures of midpoint distributions
  - examine cross-entropy, i.e., (-\frac{1}{N} \sum_{i=1}^N
    \log q(x_i) ), where q(x) is probability according to the
    model. Very important to make sure that q is normalized, which
    could be difficult.

* 0. assembling datasets
** General notes about organizing data
- filetypes:
  + x.curve - a curve
  + x.annotation - a set of possibly labeled points, corresponding to a marked image
  + x.sdf - an sdf, specified in terms of indices
  + x.edges - a curve network

  might also want:
  + x.grammar, although it's unclear how to balance expressiveness vs. readability

- dataset/clean
  dataset/annotated
  dataset/misc

  is a pretty good hierarchy, might also want to aim for having all
  files associated with a particular image have the same id number


** Articulator and other synthetic data
** Romer
*** Romer I
  - hand-annotated simplified version, with 27 points hand-marked
*** Romer II
  - ground-truth Romer curves from diff
*** Romer III
  - images
** Weizmann horses
** hand datasets
  - http://www.idiap.ch/resource/gestures/
  - http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/hand_data.html

** ASL alphabet

** LabelMe


** Time Series datasets

* 1. grammatical shape models
** TODO compare grammars to markov models
  - implement markov models (already done somewhere?)
  - parse with markov models? this is probably easy, but it would
    require a bunch of coding.
  - alternatively, we found a paper that shoehorns a markov model into
    a bingham distro or some such
** TODO PLAN compare grammars to procrustes / watson / bingham as baseline
  - need to implement whatever, which will require figuring out the
    math for it
  - can represent shapes as curves, so we just need to know how to map
    shape to procrustes-style coordinates, how to compute score (just
    a dot product?)
  - should compare to learned watson etc., so we need to be able to learn a
    watson etc.
  - need to write code to organize the cross-entropy calculation
  - need to make sure that both grammars and watson are normalized distros
  - should do a grid search over the concentration parameter, at least
    for watson. can either report all or choose one by xval
** TODO build interesting grammars by hand

Simplest is probably a simplified hand.
 - want to see choice (thumb vs. no thumb)
 - want to see shared parts (fingers)
 - want to see meaningful MP dist (ideally, articulation of
   fingers and thumbs)
 - check that samples look nice

  - if we build a model for hand-annotated romer or asl, compare a
    hand-built grammar with rich structure to an auto-generated
    one. this is not that important here, because without EM the
    structure is not that important.

** TODO build interesting and valid grammars from shapetrees
Want to have good shape deformation given simple hand-picked midpoint
models, with no structural variability whatsoever, not even X->l or
L->LL
  - use hand-built grammars based on hand-annotation and
    hand-choosing the shapetree
  - see how choosing different shape trees will influence the
    samples
  - try comparing samples to samples from a standard
    procrustes/watson/bingham model     
  - look at cross-entropy

  - what kind of dataset do we need? want enough images that the
    watson distro or whatever can actually be fit. need to have
    explicit correspondences. hand could work, or we could put
    explicit annotations on romer.

  - what code is needed? 
    - k-ary watson, need to be able to calculate probability
      (including normalization), sample, and learn
    - need to specify a single parse tree
    - need to be able to train, use, and sample from 3-ary watson,
      given hand-labelings

** TODO Figure out how to deal with variation in length
  - Either have good shape models that include X->l and L->LL (or
    figure out a different way to deal with variable length curves)

  - need to make LLL rules for some of the subcurves. if we are going
    to change this to have scaled L's, this becomes kind of scary. do
    we generate scaled L's on the fly during parsing, or do we
    generate a whole bunch of statically scaled L's during grammar
    creation, and just go down fairly far (thus making the grammar far
    bigger than it is now) a compromise would be to statically
    generate the L's but have them for a number of scales, and link
    them all up appropriately (rounding the scales a bit) that seems
    like it would work just fine.

  - again, want cross-entropy to support this, although it's not
    clear what the non-grammatical version would be

  - X->l L->LL may(???) be basically mandatory for classification or for
    cluttered parsing, both domains have length bias problems to
    consider

    - for classification, we are parsing a single curve with many
      grammars. therefore, it is important that we use the same number
      of rules in parsing the curve with each grammar. using X->l and
      L->LL makes this sort of true, since we always use n X->l rules
      and (n-1) X->YZ rules, including L->LL. The concentrations make
      this not work perfectly, since those (n-1) rules will not all
      have the same concentration, and it seems like concentration
      tells you a lot about the magnitude of the terms (but not
      everything)

      in the past we have used log P(X->l) \propto scale(X)^2, since
      we are guaranteed that sum scale(X) = 1 for the set of
      nonterminals used in any parse. EXCEPT, this does not apply to
      the leaves, since they exist at multiple scales once L->LL is
      invoked

      so maybe the answer is to have an infinite chain of nonterminals
      that AREN'T self-similar. The most obvious thing to do would be
      to have the leaves be L_s, and have L_s -> L_t L_{s-t}. 

      This leaves us with the problem of deciding the properties of
      L_s as a function of s. The probability of L_s -> l can be set
      as before, since the ell-2 norm of things that sum to 1 seemed
      pretty solid - mostly unbiased, some push towards balance

      this still leaves us with picking a midpoint distribution, and
      also with deciding P(L_s -> L_t L_{s-t}) as a function of t. We
      could simply fix t=s/2.

      Picking the midpoint distributions seems like it should just be
      done empirically. Pick a class of shapes, and just look at what
      L_s -> L_s/2 L_s/2 would look like. We can use either euclidean
      arclength or simply the index to think about the scale. To get
      enough data, we should group the scales somehow? Good scales
      are: 1/2, .4, .3, .2, .1, .09, .08, .07, ..., .01, .009, etc.
      We can look at every subcurve and just round everything to the
      nearest scale.

      This still does not address texture, but it would at least let
      us do our classification in a principled way.

      This might even get at texture, since it gets relatively close
      to the GP "correlation at a specific distance" phenomenon.

      results: there is an interesting amount of variation between
      classes in swedish leaves, very different watson concentrations,
      slightly different patterns wrt scale

  - next thing to do: sample from this somehow, see if we like the
    generated subcurves

  - ultimately, can bottom out the single-example grammars in this
    way, sample from them, see what happens. it seems like different
    classes would switch from shape to texture at different scales.

    we could even explicitly allow a choice for this, i.e., have L->LL
    rules even for nonterminals that do have rules. then EM could try
    to decide about the global/local decision for us (although EM is
    completely untrustable!!!!!)

  - a good start would be to just do some exploratory work, figure out
    what short curves tend to look like, then we know more about things...
    
** TODO Have good shape models using more complex grammars
    - try building them by hand by hand-parsing example curves,
      choosing intuitively reasonable correspondences.
    - imposing a hand-built grammar on Romer seems relatively
      reasonable, especially if we hand-pick and use the ground truth
      curves
    - can also impose a hand-built grammar on ASL

      
* 2. Parsing
** DONE Recover a 1-1 correspondence   
  - show the parse table? it will be 27^3 entries... could just show
    the scores for the 27 rotations.

  - do this with more than one curve!
** TODO Recover a 1-1 correspondence with extra intermediate points
  - given curves with corresponding points, and also more intermediate
    points, make sure that we can recover the correspondence. already
    we are faced with the length problem

  - do this with swedish leaves or some other relatively rigid
    dataset, even easy romer is relatively hard if we're going from a
    single example.

  - estimate a grammar from all but one of the easy romer curves
    (using given correspondence), then try to parse a non-easy romer
    curve (subsampled to 50 or so?)

** TODO Recover a 1-1 correspondence with misleading intermediate points
  - given curves with corresponding points, and also somewhat
    misleading intermediate points, make sure that we can recover the
    correspondence
    - want to see ambiguity (fake stubby finger parsed by L->LL or some such)
** TODO Recover a correspondence where some points are missing 
  - given curves with corresponding points, where some may be missing,
    make sure that we can recover the correspondence
** TODO Recover a correspondence with both extra points and missing points
  - given curves with corresponding points, where there are both extra
    points and missing points, make sure that we can recover the
    correspondence
** TODO Given hand-built rich grammar, choose correct structure
  - given a hand-built grammar with structural variation, make sure
    that parsing chooses the correct structure, and also gets the
    corresponding points correct


* 3. EM for parameter tuning / shape learning
General notes: want to do each goal for both a hand-built and
auto-generated single-example grammars.

** TODO PLAN given fixed parses and hand-selected grammar, EM retrains midpoint distros well
 - [ ] take 1 curve
 - [ ] impose perfect grammar,
 - [ ] parse the other curve
 - [ ] reestimate midpoints
      
** TODO given fixed parses, EM retrains midpoint distros well
  - retrain various single-example grammars by using fixed parses of
    similar curves

** TODO given fixed parses, EM tunes length-related rules well
  - length-related: L->LL and X->l. 
  - this is a retarded goal, since these parameters are essentially
    just measures of scale, and thus it is not very meaningful to
    learn them

** TODO given fixed parses, EM tunes rich grammars correctly
  - this should already work, just verifying that EM behaves
    correctly given fixed parses

** TODO given bad parses, EM fails in some way
  - impose bad grammar, see what happens


** TODO EM retrains midpoint distros well

** TODO EM tunes length-related rules well
  - again, retarded

** TODO WORKING EM tunes grammars well
  - test_em.ml is at least trying to do this

  - it works!
    
    we can see that we are being hurt by the unimodality of the watson
    distro. this suggests that we could try mixtures. we could also do
    that on the cheap by duplicating the rules and perturbing the mean
    shapes differently on the same rule

    to do this, enliven and then walk over the compositions, inserting
    duplicates as we go. watson.ml could have a jittering function.

** TODO EM tunes rich grammars correctly
  - basically just making sure that if parsing works on its own, and
    retraining works on its own, then we can combine them: we both
    find the correct parses, and are sure enough about them to do our
    updates correctly

  - think about discriminative training vs. EM



* 4. Parsing in Real Images
** TODO Parse cluttered image with hand-built grammar, localization information?
  - GOAL: be able to parse from a cluttered image, using a hand-built
    grammar, given lots of localization information

** TODO Parse cluttered image with hand-built grammar
  - GOAL: be able to parse from a cluttered image using a hand-built
    grammar

** TODO Parse cluttered image with auto-generated grammar
  - GOAL: be able to parse from a cluttered image using an
    auto-generated grammar

** TODO Parse cluttered image with hand-built rich grammar, get pose info
  - GOAL: be able to detect pose information from a cluttered image
    using a hand-built rich grammar

** TODO Tune hand-built grammar with hand-parsed cluttered images
  - GOAL: be able to use hand-picked parses from cluttered images to
    tune a hand-built grammar, possibly discriminatively

** TODO Tune hand-built grammar with cluttered images 
  - GOAL: be able to use parses from cluttered images to tune a
    hand-built grammar

** TODO Tune auto-generated grammar with cluttered images
  - GOAL: be able to use parses from cluttered images to tune an
    auto-generated grammar

** TODO Improve 2-D parsing with image filters with hand-picked grammars, keypoints
  - look at a small window around the point, and use this to know
    where various points are. Use this to more accurately parse ASL
    images. at this point we are tackling a special case of a pushpin
    grammar. (where the pins are connected via a shape grammar rather
    than some other model) Do this with hand-picked keypoints.

** TODO Improve 2-D parsing with image filters with hand-picked grammars, auto keypoints
  - As above, but try to pick keypoints automatically. That is, take
    images with ground-truth silhouettes, and try to simplify these to
    a few points such that the curve is still approximately
    represented, and such that the points are at distinctive
    locations, e.g. look more or less like SIFT keypoints.

** TODO Improve 2-D parsing with image filters with auto grammar, auto keypoints

** More general pushpin grammars?
  - do something with more general pushpin grammars? can have some
    arrangement of pushpins tied together with procrustes models. that
    is, can grow existing set of pushpins by imposing a procrustes
    model on some collection of old and new points (in the normal
    case, two old points and one new point)

** Do detection and segmentation on real images
*** With working EM
 - [ ] Filter out most false positives with Pedro's hog model
 - [ ] Run pose-estimating detector as a benchmark, mark pixels according to rectangles
 - [ ] Parse with model grammar to filter out more false positives, mark pixels according to MAP curve
*** With working structure learning

** Foreground detection
 - Look at Pedro's thesis
 - Sample from the posterior using the inside weights
 - Can have a lot of false detections and a good filtration
   algorithm - sampling is cheap compared to parsing
 - Can look at a slightly more complicated version of the generic grammar from Pedro's thesis


* 5. Using SDF's in other domains
thoughts: can we turn any binary decomposition of a string into an
SDF, using Pedro's construction?

can we derive a lwoer bound on cost of any parse using sdf parses?

we can imagine trimming any parse tree by intersecting every interval
in it with a particular interval. the question then becomes, if T_1
gets trimmed to T_2, and T_3 gets trimmed to T_4, and T_1 and T_3
compose to give T, how can we know about that?

we could also look at parsing where we try to optimize density, or
just optimize X->>[i,j] for each length of observed yield

if we know that X->YZ, and
Y ->> [ ?i, <=j ] and Z ->> [ <=j, ??k ], then we *might* have X ->> [ ?i, ??k ] 

can think more generally of assertions X ->> [ I,J ] where I,J are
sets. Then Y ->> [I,J], Z ->> [K,L], and J,K not disjoint, then we can have X ->> [I,L]

also, if i in I, then X -> a, data[i]=a, can deduce X ->> [{i},{i+1}]

also can deduce X->>[I,J] |= X->>[I',J'] if I subset I', J subset J'

guarantee is cost~ <= cost, i.e.
think of cost~(X->>[I,J]) = theta as an assertion that cost(X->>[i,j]) >= theta for any i in I, j in J

rephrasing, cost~(X->>[I,J]) <= min_{i in I, j in J} cost(X->>[i,j]) 

can also look at cost~ >= cost, this has false negatives instead of false positives

other random thought - maybe we can turn any binary decomposition into
an SDF via pedro's construction, we could even do that with 2-d stuff
like a hierarchical segmentation. 

** TODO Improve on time-series classification with SDF's

* 6. Learning Structure
** TODO Figure out optimal single-example grammar
  - figure out the correct way to build a grammar from a single example
    - random thought: what if we formulate some notion of
      triangle-skinniness, and use this to define the optimal
      subtree. this seems like it would help with a lot of
      issues. ratio of shortest to longest side is one measure, maybe
      we would add logs of that

  - we can optimize any function of the form sum_{examples}
    sum_{i,j,k} f(i,j,k) if we let f(i,j,k) be the negative log
    probability of the shape deformation cost (which we know because
    we have correspondences) then we can get cross-entropy this way

  - we are getting some fucking constituents in this bitch!

** TODO Implement Merge and Replace
  - demonstrate that merging and replacement do something reasonable,
    given an auto-generated grammar
  - start from ideal single use grammar, show a Replace (finger models)
  - start from ideal single use grammar, show a Merge (thumb vs no thumb)

** TODO Implement Merge and Replace KL heuristics
  - actually compute the KL tables for these two guys
  - demonstrate that merging and replacement heuristics do something
    reasonable, given hand-built grammar

** TODO Use Merge and Replace to search for good grammar 
  - demonstrate that we can learn interesting grammars from scratch,
    i.e., that beam search or whatever works well given the
    heuristics. probably have to do something more clever than
    applying individual merges and replacements based on pairwise
    similarity.

  - using ASL alphabet seems like it gives a lot of opportunities for 
    interesting grammars

  - can hope to learn symmetries of human figure
  - sample a shape and decide whether it looks plausible
  - generate novel but correct shapes?


** Figure out how to optimally incorporate new samples

* 7. Learning Curve Texture
** TODO Give a grammar that captures only texture, sample
  - set up some hierarchy of scales, with decompositions between them
    - would like to use all the data we can get, which means we want
      every length of curve to be close to some scale 
  - build a grammar from this
  - learn midpoint distributions by going over all pairs of curve
    points and taking the midpoint (and maybe other percentiles) by
    arclength to get triangles
  - sample from it



** TODO Come up with curve texture descriptor that does OK on swedish leaves
  - current thoughts: think of a curve as coming from a gaussian
    process. map to modified bookstein coordinates, subtract out some
    global trend (perhaps the optimal parabola centered midway, e.g.)
    and then figure out what the covariance of f(x_1) and f(x_2) is as
    a function of x_1 - x_2. Graph this as a function of dx to see if
    anything pops out, it should for various sawtooth-like curves

** TODO Improve classification performance of global model with texture model

* 8. Classification
** Use a discriminative version of EM?
** Distinguish between dog silhouettes and cat silouettes?

