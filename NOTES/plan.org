* notes on code organization

  - in general, each file should do one thing. right now there is too
    much of a tendency towards monolithic objects.

  - break the link between sdf and curve, think of sdf as purely being
    about indices. either stop precomputing shape descriptor, or store
    those separately?

  - simplify the code relating to abstract grammars, such that there
    is only one (possibly parameterized) type of grammar. make it so
    that sdf and shape grammar both include a grammar, in a more
    straightforward way
    - as part of this, maybe take in some functions that do particular
      things on the grammar?

    - live_shape_grammar: what does this do? it wraps live grammar,
      and adds convenience functions for insertion. this is pretty
      cool - maybe it should be how all live grammars work

    - what's the deal with live grammars and frozen grammars? we went
      to a lot of trouble to make sure that they both supported
      certain stuff so that we could e.g. parse with a live grammar,
      but that seems like it's just a waste. live grammars should be
      for building, and frozen grammars should be for using

    - at some point, would be good to solve the start/0 issue

    - at some point, would be good to have it figure out the correct
      ordering of symbols etc. instead of having to do things in a
      particular order

    - how do we solve the problem of having these nonterminals with
      associated data? if we want to make it so that there is only one
      sort of grammar, then shape grammars and sdf's are both either
      parameterized variants of grammars, or a grammar plus a way of
      looking up the relevant metadata in e.g. a hash table.

      it's nice to interact with nonterminals as unified objects, so
      looking up the metadata in a hash table is probably bad, and
      there could be problems keeping things in sync

      if they are unified things, then we probably need to have a
      nested record structure, so that we can separate the parts that
      hook into the grammar and the parts that are actually metadata

      symbol.sid
      symbol.dcompids
      symbol.lcompids
      symbol.sdata.closed
      symbol.sdata.straightprob

      in the past, something has felt vaguely weird about sdata. it is
      probably partly that we have been trying to maintain too much
      flexibility about what's in there, so we haven't been dealing
      with it straightforwardly. so, more or less fixing the content
      of sdata would be good, should probably be the union of what's
      in Grammar.symbol and Sdf.debug_sdata

    - what is up with get_params? it is basically a bunch of options
      about how we build a grammar, but i think it is too specific to
      a particular view of it. it would be better to just take some
      functions, but what would they be?

      - need(?) nonsense symbol and production to init live grammar
      - need to make a symbol from every subcurve, which entails
        picking straightcost, and getting debug info like the subcurve

      - need to make a rule from every production of sdf, which
        entails picking a shape model, getting debug info, getting
        sids

      - need to make LLL rules for some of the subcurves. if we are
        going to change this to have scaled L's, this becomes kind of
        scary. do we generate scaled L's on the fly during parsing, or
        do we generate a whole bunch of statically scaled L's during
        grammar creation, and just go down fairly far (thus making the
        grammar far bigger than it is now) a compromise would be to
        statically generate the L's but have them for a number of
        scales, and link them all up appropriately (rounding the
        scales a bit) that seems like it would work just fine.

    - replace/merge/sample/prune/drawing might need to change very
      slightly if the typedefs change, but not significantly

    - a lot of drawing functions feel more like experiments and less
      like library functions. they have a lot of arbitrary choices and
      they are usually tailored to some particular use of them. it's
      not clear what to do about this though

  - general rule of thumb(?) - the library files should not have
    serious choices in them, they should give enough support for the
    experiments and executables to make choices. when a choice is
    needed, take a relatively generic function instead of various
    parameters. this is good for keeping the library current and
    correct, and as long as we don't change the sort of function we
    accept, it also means that old experiments will still run, even if
    we have moved on to different choices in newer experiments

  - rename curve_network maybe? think about the data structure in there

  - think about moving geometry into basically a module about complex #s

  - shape.ml is kind of a flaky file... there's very little in it
    though, so maybe that's ok

* METRICS
  - examine samples
  - examine pictures of midpoint distributions
  - examine cross-entropy, i.e., (-\frac{1}{N} \sum_{i=1}^N
    \log q(x_i) ), where q(x) is probability according to the
    model. Very important to make sure that q is normalized, which
    could be difficult.

* 0. assembling datasets
** General notes about organizing data
- filetypes:
  + x.curve - a curve
  + x.annotation - a set of possibly labeled points, corresponding to a marked image
  + x.sdf - an sdf, specified in terms of indices
  + x.edges - a curve network

  might also want:
  + x.grammar, although it's unclear how to balance expressiveness vs. readability

- dataset/clean
  dataset/annotated
  dataset/misc

  is a pretty good hierarchy, might also want to aim for having all
  files associated with a particular image have the same id number


** Articulator and other synthetic data
** Romer
*** Romer I
  - hand-annotated simplified version, with 27 points hand-marked
*** Romer II
  - ground-truth Romer curves from diff
*** Romer III
  - images
** Weizmann horses
** hand datasets
  - http://www.idiap.ch/resource/gestures/
  - http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/hand_data.html

** ASL alphabet

** LabelMe


** Time Series datasets

* 1. grammatical shape models
** TODO compare grammars to markov models
  - implement markov models (already done somewhere?)
  - parse with markov models? this is probably easy, but it would
    require a bunch of coding.
  - alternatively, we found a paper that shoehorns a markov model into
    a bingham distro or some such
** TODO PLAN compare grammars to procrustes / watson / bingham as baseline
  - need to implement whatever, which will require figuring out the
    math for it
  - can represent shapes as curves, so we just need to know how to map
    shape to procrustes-style coordinates, how to compute score (just
    a dot product?)
  - should compare to learned watson etc., so we need to be able to learn a
    watson etc.
  - need to write code to organize the cross-entropy calculation
  - need to make sure that both grammars and watson are normalized distros
  - should do a grid search over the concentration parameter, at least
    for watson. can either report all or choose one by xval
** TODO build interesting grammars by hand

Simplest is probably a simplified hand.
 - want to see choice (thumb vs. no thumb)
 - want to see shared parts (fingers)
 - want to see meaningful MP dist (ideally, articulation of
   fingers and thumbs)
 - check that samples look nice

  - if we build a model for hand-annotated romer or asl, compare a
    hand-built grammar with rich structure to an auto-generated
    one. this is not that important here, because without EM the
    structure is not that important.

** TODO build interesting and valid grammars from shapetrees
Want to have good shape deformation given simple hand-picked midpoint
models, with no structural variability whatsoever, not even X->l or
L->LL
  - use hand-built grammars based on hand-annotation and
    hand-choosing the shapetree
  - see how choosing different shape trees will influence the
    samples
  - try comparing samples to samples from a standard
    procrustes/watson/bingham model     
  - look at cross-entropy

  - what kind of dataset do we need? want enough images that the
    watson distro or whatever can actually be fit. need to have
    explicit correspondences. hand could work, or we could put
    explicit annotations on romer.

  - what code is needed? 
    - k-ary watson, need to be able to calculate probability
      (including normalization), sample, and learn
    - need to specify a single parse tree
    - need to be able to train, use, and sample from 3-ary watson,
      given hand-labelings

** TODO Figure out how to deal with variation in length
Either have good shape models that include X->l and L->LL (or figure
out a different way to deal with variable length curves)
  - again, want cross-entropy to support this, although it's not
    clear what the non-grammatical version would be

  - X->l L->LL may(???) be basically mandatory for classification or for
    cluttered parsing, both domains have length bias problems to
    consider

    - for classification, we are parsing a single curve with many
      grammars. therefore, it is important that we use the same number
      of rules in parsing the curve with each grammar. using X->l and
      L->LL makes this sort of true, since we always use n X->l rules
      and (n-1) X->YZ rules, including L->LL. The concentrations make
      this not work perfectly, since those (n-1) rules will not all
      have the same concentration, and it seems like concentration
      tells you a lot about the magnitude of the terms (but not
      everything)

      in the past we have used log P(X->l) \propto scale(X)^2, since
      we are guaranteed that sum scale(X) = 1 for the set of
      nonterminals used in any parse. EXCEPT, this does not apply to
      the leaves, since they exist at multiple scales once L->LL is
      invoked

      so maybe the answer is to have an infinite chain of nonterminals
      that AREN'T self-similar. The most obvious thing to do would be
      to have the leaves be L_s, and have L_s -> L_t L_{s-t}. 

      This leaves us with the problem of deciding the properties of
      L_s as a function of s. The probability of L_s -> l can be set
      as before, since the ell-2 norm of things that sum to 1 seemed
      pretty solid - mostly unbiased, some push towards balance

      this still leaves us with picking a midpoint distribution, and
      also with deciding P(L_s -> L_t L_{s-t}) as a function of t. We
      could simply fix t=s/2.

      Picking the midpoint distributions seems like it should just be
      done empirically. Pick a class of shapes, and just look at what
      L_s -> L_s/2 L_s/2 would look like. We can use either euclidean
      arclength or simply the index to think about the scale. To get
      enough data, we should group the scales somehow? Good scales
      are: 1/2, .4, .3, .2, .1, .09, .08, .07, ..., .01, .009, etc.
      We can look at every subcurve and just round everything to the
      nearest scale.

      This still does not address texture, but it would at least let
      us do our classification in a principled way.

      This might even get at texture, since it gets relatively close
      to the GP "correlation at a specific distance" phenomenon.

      results: there is an interesting amount of variation between
      classes in swedish leaves, very different watson concentrations,
      slightly different patterns wrt scale

  - next thing to do: sample from this somehow, see if we like the
    generated subcurves

  - a good start would be to just do some exploratory work, figure out
    what short curves tend to look like, then we know more about things...


â
** TODO Have good shape models using more complex grammars
    - try building them by hand by hand-parsing example curves,
      choosing intuitively reasonable correspondences.
    - imposing a hand-built grammar on Romer seems relatively
      reasonable, especially if we hand-pick and use the ground truth
      curves
    - can also impose a hand-built grammar on ASL

      
* 2. Parsing
** DONE Recover a 1-1 correspondence   
  - show the parse table? it will be 27^3 entries... could just show
    the scores for the 27 rotations.

  - do this with more than one curve!
** TODO Recover a 1-1 correspondence with extra intermediate points
  - given curves with corresponding points, and also more intermediate
    points, make sure that we can recover the correspondence. already
    we are faced with the length problem

  - do this with swedish leaves or some other relatively rigid
    dataset, even easy romer is relatively hard if we're going from a
    single example.

  - estimate a grammar from all but one of the easy romer curves
    (using given correspondence), then try to parse a non-easy romer
    curve (subsampled to 50 or so?)

** TODO Recover a 1-1 correspondence with misleading intermediate points
  - given curves with corresponding points, and also somewhat
    misleading intermediate points, make sure that we can recover the
    correspondence
    - want to see ambiguity (fake stubby finger parsed by L->LL or some such)
** TODO Recover a correspondence where some points are missing 
  - given curves with corresponding points, where some may be missing,
    make sure that we can recover the correspondence
** TODO Recover a correspondence with both extra points and missing points
  - given curves with corresponding points, where there are both extra
    points and missing points, make sure that we can recover the
    correspondence
** TODO Given hand-built rich grammar, choose correct structure
  - given a hand-built grammar with structural variation, make sure
    that parsing chooses the correct structure, and also gets the
    corresponding points correct


* 3. EM for parameter tuning / shape learning
General notes: want to do each goal for both a hand-built and
auto-generated single-example grammars.

** TODO PLAN given fixed parses and hand-selected grammar, EM retrains midpoint distros well
 - [ ] take 1 curve
 - [ ] impose perfect grammar,
 - [ ] parse the other curve
 - [ ] reestimate midpoints
      
** TODO given fixed parses, EM retrains midpoint distros well
  - retrain various single-example grammars by using fixed parses of
    similar curves

** TODO given fixed parses, EM tunes length-related rules well
  - length-related: L->LL and X->l. 
  - this is a retarded goal, since these parameters are essentially
    just measures of scale, and thus it is not very meaningful to
    learn them

** TODO given fixed parses, EM tunes rich grammars correctly
  - this should already work, just verifying that EM behaves
    correctly given fixed parses

** TODO given bad parses, EM fails in some way
  - impose bad grammar, see what happens


** TODO EM retrains midpoint distros well

** TODO EM tunes length-related rules well
  - again, retardedo  

** TODO EM tunes rich grammars correctly
  - basically just making sure that if parsing works on its own, and
    retraining works on its own, then we can combine them: we both
    find the correct parses, and are sure enough about them to do our
    updates correctly

  - think about discriminative training vs. EM



* 4. Parsing in Real Images
** TODO Parse cluttered image with hand-built grammar, localization information?
  - GOAL: be able to parse from a cluttered image, using a hand-built
    grammar, given lots of localization information

** TODO Parse cluttered image with hand-built grammar
  - GOAL: be able to parse from a cluttered image using a hand-built
    grammar

** TODO Parse cluttered image with auto-generated grammar
  - GOAL: be able to parse from a cluttered image using an
    auto-generated grammar

** TODO Parse cluttered image with hand-built rich grammar, get pose info
  - GOAL: be able to detect pose information from a cluttered image
    using a hand-built rich grammar

** TODO Tune hand-built grammar with hand-parsed cluttered images
  - GOAL: be able to use hand-picked parses from cluttered images to
    tune a hand-built grammar, possibly discriminatively

** TODO Tune hand-built grammar with cluttered images 
  - GOAL: be able to use parses from cluttered images to tune a
    hand-built grammar

** TODO Tune auto-generated grammar with cluttered images
  - GOAL: be able to use parses from cluttered images to tune an
    auto-generated grammar

** TODO Improve 2-D parsing with image filters with hand-picked grammars, keypoints
  - look at a small window around the point, and use this to know
    where various points are. Use this to more accurately parse ASL
    images. at this point we are tackling a special case of a pushpin
    grammar. (where the pins are connected via a shape grammar rather
    than some other model) Do this with hand-picked keypoints.

** TODO Improve 2-D parsing with image filters with hand-picked grammars, auto keypoints
  - As above, but try to pick keypoints automatically. That is, take
    images with ground-truth silhouettes, and try to simplify these to
    a few points such that the curve is still approximately
    represented, and such that the points are at distinctive
    locations, e.g. look more or less like SIFT keypoints.

** TODO Improve 2-D parsing with image filters with auto grammar, auto keypoints

** More general pushpin grammars?
  - do something with more general pushpin grammars? can have some
    arrangement of pushpins tied together with procrustes models. that
    is, can grow existing set of pushpins by imposing a procrustes
    model on some collection of old and new points (in the normal
    case, two old points and one new point)

** Do detection and segmentation on real images
*** With working EM
 - [ ] Filter out most false positives with Pedro's hog model
 - [ ] Run pose-estimating detector as a benchmark, mark pixels according to rectangles
 - [ ] Parse with model grammar to filter out more false positives, mark pixels according to MAP curve
*** With working structure learning

** Foreground detection
 - Look at Pedro's thesis
 - Sample from the posterior using the inside weights
 - Can have a lot of false detections and a good filtration
   algorithm - sampling is cheap compared to parsing
 - Can look at a slightly more complicated version of the generic grammar from Pedro's thesis


* 5. Using SDF's in other domains
thoughts: can we turn any binary decomposition of a string into an
SDF, using Pedro's construction?

can we derive a lwoer bound on cost of any parse using sdf parses?

we can imagine trimming any parse tree by intersecting every interval
in it with a particular interval. the question then becomes, if T_1
gets trimmed to T_2, and T_3 gets trimmed to T_4, and T_1 and T_3
compose to give T, how can we know about that?

we could also look at parsing where we try to optimize density, or
just optimize X->>[i,j] for each length of observed yield

if we know that X->YZ, and
Y ->> [ ?i, <=j ] and Z ->> [ <=j, ??k ], then we *might* have X ->> [ ?i, ??k ] 

can think more generally of assertions X ->> [ I,J ] where I,J are
sets. Then Y ->> [I,J], Z ->> [K,L], and J,K not disjoint, then we can have X ->> [I,L]

also, if i in I, then X -> a, data[i]=a, can deduce X ->> [{i},{i+1}]

also can deduce X->>[I,J] |= X->>[I',J'] if I subset I', J subset J'

guarantee is cost~ <= cost, i.e.
think of cost~(X->>[I,J]) = theta as an assertion that cost(X->>[i,j]) >= theta for any i in I, j in J

rephrasing, cost~(X->>[I,J]) <= min_{i in I, j in J} cost(X->>[i,j]) 

can also look at cost~ >= cost, this has false negatives instead of false positives

other random thought - maybe we can turn any binary decomposition into
an SDF via pedro's construction, we could even do that with 2-d stuff
like a hierarchical segmentation. 

** TODO Improve on time-series classification with SDF's

* 6. Learning Structure
** TODO Figure out optimal single-example grammar
  - figure out the correct way to build a grammar from a single example
    - random thought: what if we formulate some notion of
      triangle-skinniness, and use this to define the optimal
      subtree. this seems like it would help with a lot of
      issues. ratio of shortest to longest side is one measure, maybe
      we would add logs of that

  - we can optimize any function of the form sum_{examples}
    sum_{i,j,k} f(i,j,k) if we let f(i,j,k) be the negative log
    probability of the shape deformation cost (which we know because
    we have correspondences) then we can get cross-entropy this way

  - we are getting some fucking constituents in this bitch!

** TODO Implement Merge and Replace
  - demonstrate that merging and replacement do something reasonable,
    given an auto-generated grammar
  - start from ideal single use grammar, show a Replace (finger models)
  - start from ideal single use grammar, show a Merge (thumb vs no thumb)

** TODO Implement Merge and Replace KL heuristics
  - actually compute the KL tables for these two guys
  - demonstrate that merging and replacement heuristics do something
    reasonable, given hand-built grammar

** TODO Use Merge and Replace to search for good grammar 
  - demonstrate that we can learn interesting grammars from scratch,
    i.e., that beam search or whatever works well given the
    heuristics. probably have to do something more clever than
    applying individual merges and replacements based on pairwise
    similarity.

  - using ASL alphabet seems like it gives a lot of opportunities for 
    interesting grammars

  - can hope to learn symmetries of human figure
  - sample a shape and decide whether it looks plausible
  - generate novel but correct shapes?


** Figure out how to optimally incorporate new samples

* 7. Learning Curve Texture
** TODO Come up with curve texture descriptor that does OK on swedish leaves
  - current thoughts: think of a curve as coming from a gaussian
    process. map to modified bookstein coordinates, subtract out some
    global trend (perhaps the optimal parabola centered midway, e.g.)
    and then figure out what the covariance of f(x_1) and f(x_2) is as
    a function of x_1 - x_2. Graph this as a function of dx to see if
    anything pops out, it should for various sawtooth-like curves

** TODO Improve classification performance of global model with texture model

* 8. Classification
** Use a discriminative version of EM?
** Distinguish between dog silhouettes and cat silouettes?

